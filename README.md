# ğŸ™ï¸ Speaker Diarization Enhancement Using Denoising DL-Model
**ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¡ìŒ ì œê±° ëª¨ë¸ì„ í™œìš©í•œ í™”ì ë¶„ë¦¬ ì •í™•ë„ í–¥ìƒ ì‹œìŠ¤í…œ**

## ğŸ§© í”„ë¡œì íŠ¸ í•œ ì¤„ ìš”ì•½
**ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¡ìŒ ì œê±°(denoising) + ë“€ì–¼ ì†ŒìŠ¤ VAD í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ í†µí•´ ì‹¤ì œ êµì‹¤/íšŒì˜ í™˜ê²½ì—ì„œë„ DERì„ ë‚®ì¶˜ ê³ ì •í™•ë„ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ**

## ğŸ”— ê¸°ì¡´ í”„ë¡œì íŠ¸ ë° ì°¸ê³  ë§í¬
- **ê¸°ì¡´ í”„ë¡œì íŠ¸ GitHub:** [nemo-multistage-classroom-diarization](https://github.com/EduNLP/nemo-multistage-classroom-diarization.git)
- **Deep Learning Model GitHub:** [DeepFilterNet](https://github.com/Rikorose/DeepFilterNet)
- **ì°¸ê³  ë…¼ë¬¸:** [EDM 2025 - Multistage Classroom Diarization](https://educationaldatamining.org/edm2025/proceedings/2025.EDM.short-papers.199/)  

## ğŸ“˜ Overview  

### ğŸ¯ ë¬¸ì œì   
ê¸°ì¡´ì˜ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œì€ **ì‹œë„ëŸ½ê³  ë‹¤ì–‘í•œ ì†ŒìŒì´ ì¡´ì¬í•˜ëŠ” êµì‹¤Â·íšŒì˜ í™˜ê²½**ì—ì„œ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë¨.  

### ğŸ’¡ í•´ê²°ë°©ì•ˆ
ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒì„ ê²°í•©í•œ **ë‹¤ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸**ì„ ì œì•ˆí•¨.  
1. **ë”¥ëŸ¬ë‹ ê¸°ë°˜ Speech Enhancement (DeepFilterNet)**
   - ì¡ìŒ ì–µì œ + í™”ì ìŒìƒ‰ ë³´ì¡´  
2. **ë“€ì–¼ ì†ŒìŠ¤ VAD (wav2vec2 + Whisper)**  
   - ì†ŒìŒ í™˜ê²½ì—ì„œë„ ëª…í™•í•œ ë°œí™” êµ¬ê°„ ê²€ì¶œ  
3. **NeMo ê¸°ë°˜ Speaker Embedding + Clustering + Labeling**  
   - ê¹¨ë—í•œ ì˜¤ë””ì˜¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™”ì ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ  
   - DER(Diarization Error Rate) ê°ì†Œ  

## ğŸ§  System Pipeline 

```
Noisy Audio
     â†“
[Phase 1] Deep Learning Speech Enhancement (DeepFilterNet V3)
     â†“
[Phase 2] Dual-Source VAD (wav2vec2 + Whisper)
     â†“
[Phase 3] VAD Fusion & Segmentation
     â†“
[Phase 4] Speaker Embedding & Clustering (NeMo)
     â†“
[Phase 5] Speaker Labeling
     â†“
Enhanced & Tagged Audio Output
```

---

## ğŸ’» Demo  

### ğŸ§ Input Example
```
classbank_audio_data/audio/2.wav
```

### âš™ï¸ Output Example
```
diarization_output/pred_rttms/2_denoised_diarized.rttm
vad_outs.json
```

| File | Description |
|------|--------------|
| `.wav` | ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ |
| `.json` | VAD ê²°ê³¼ (ìŒì„± êµ¬ê°„ ì •ë³´) |
| `.rttm` | í™”ì ë¶„ë¦¬ ê²°ê³¼ (who spoke when) |

## ğŸ“ˆ Result & Performance  

### ğŸ§® í‰ê°€ ì§€í‘œ (DER)
```
DER = (FA + MISS + CER) / Duration
```
| Metric | ì˜ë¯¸ |
|---------|------|
| FA (False Alarm) | ë°œí™” ì—†ìŒ â†’ ìˆìŒìœ¼ë¡œ ì˜¤íƒ |
| MISS | ì‹¤ì œ ë°œí™” â†’ ë¯¸íƒì§€ |
| CER (Confusion Error Rate) | ë°œí™”ëŠ” íƒì§€í–ˆìœ¼ë‚˜ í™”ì í• ë‹¹ ì˜¤ë¥˜ |

### ğŸ“Š ê²°ê³¼
- ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ëŒ€ë¹„ DER ê°ì†Œ
- Whisper + wav2vec2 ë³‘í•© ì‹œ ì•ˆì •ì  ë°œí™” ê²€ì¶œ í–¥ìƒ
- ì¡ìŒ í™˜ê²½ ê°•ê±´ì„± í–¥ìƒ  

## âš™ï¸ Installation  
- OS: linux ubuntu22.04.5 LTS
- GPU Recommand

```bash
# 1. Clone repository
git clone https://github.com/jth1097/Deep-learning-preproceng-pipelinssie-speaker-diarization-system.git
cd Deep-learning-preproceng-pipelinssie-speaker-diarization-system
rm -rf NeMo
git clone https://github.com/NVIDIA/NeMo.git

# 2. Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate     # (Windows: .venv\Scripts\activate)

# 3. Install dependencies
pip install nunpy
pip install typting-extension
pip install -r requirements.txt

# 4. Fail
./.venv/src/kenlm/python/BuildStandalone.cmake # cmake_minimum_required(VERSION 3.1) => 3.5
./.venv/src/kenlm/CMakeLists.txt # cmake_minimum_required(VERSION 3.5) => 3.5
pip install -r requirements.txt
```

## ğŸš€ Usage  

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
chnod +x run.sh
./run.sh
```

- `manifests/test.json` : ì˜¤ë””ì˜¤ ê²½ë¡œ ë° ë©”íƒ€ë°ì´í„° ëª©ë¡  
- `vad_outs.json` : VAD ê²°ê³¼ ì¤‘ê°„ ì‚°ì¶œë¬¼  
- `diarization_output` : ìµœì¢… í™”ì ë¶„ë¦¬ ê²°ê³¼ ì €ì¥ í´ë”  

## âš ï¸ Common Issues & Solutions  

| ë¬¸ì œ | ì›ì¸ | í•´ê²° ë°©ë²• |
|------|------|------------|
| `PySoundFile failed` | libsndfile ë¯¸ì„¤ì¹˜ | `sudo apt install libsndfile1` |
| CUDA ì˜¤ë¥˜ (`device not found`) | GPU í™˜ê²½ ë¯¸ì„¤ì • | CUDA 11.8 + cuDNN 8.6 ë²„ì „ í™•ì¸ |
| VAD ê²°ê³¼ ì—†ìŒ | ì…ë ¥ íŒŒì¼ í˜•ì‹ ë¶ˆì¼ì¹˜ | 16kHz mono PCM í˜•ì‹ìœ¼ë¡œ ë³€í™˜ |
| 0 bytes output | ffmpeg ë³€í™˜ ì‹¤íŒ¨ | `ffmpeg -i input.wav -ar 16000 -ac 1 output.wav` ë¡œ ì¬ìƒì„± |
| ë©”ëª¨ë¦¬ ë¶€ì¡± | ë”¥ëŸ¬ë‹ ëª¨ë¸ ë©”ëª¨ë¦¬ ì´ˆê³¼ | `--batch_size` ê°ì†Œ ë˜ëŠ” GPU ë©”ëª¨ë¦¬ ì¦ê°€ í•„ìš” |


## ğŸ§© Future Work  
- ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ ëª¨ë¸ **Fine-tuning** (ì¡ìŒ í¬í•¨ vs ì œê±° ë°ì´í„° ë³‘í•© í•™ìŠµ)  
- ë¼ë²¨ë§ + Audio-to-Text ì—°ë™ìœ¼ë¡œ **ì‹œê°ì  í™”ì êµ¬ë¶„ ìë£Œ ìƒì„±**  
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™˜ê²½ ì ìš© (on-device inference ìµœì í™”)


## ğŸ‘¥ Team â€œAloneâ€
| ì—­í•  | ì´ë¦„ |
|------|------|
| Researcher |  ì‹ í™ê·œ |
| Researcher |  ë‚¨ê²½ì‹ |
| Researcher |  ì–‘í‰í™” |
| Researcher |  ì¥íƒœí™˜ |


## ğŸ§¾ License  
This project is for **academic research** purposes under the **Konkuk University Capstone Design (ì¡¸ì—…í”„ë¡œì íŠ¸)** program.  
For any citation or reuse, please credit:  
> *ALONE et al., â€œSpeaker diarization enhancement using denoising DL-modelâ€, Konkuk Univ., 2025.*
